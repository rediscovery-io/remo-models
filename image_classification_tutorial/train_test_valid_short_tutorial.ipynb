{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification Pipeline using Remo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, Remo will be used to accelerate the process of building a transfer learning pipeline for the task of Image Classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "EI72wutnsdyO",
    "outputId": "347b3820-f40b-4707-c6c9-666680d951fc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import pandas as pd\n",
    "import os\n",
    "import tqdm\n",
    "import torch.optim as optim\n",
    "import random\n",
    "#import remo\n",
    "remo.set_viewer('jupyter')\n",
    "from pprint import pprint\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "random.seed(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Data to Remo\n",
    "\n",
    "- The dataset used in this example is the <a href=\"http://www.robots.ox.ac.uk/~vgg/data/flowers/102/\">Flowers 102 Dataset</a>.\n",
    "- Run the next cell to download the data from s3, create a new folder and extract the files required.\n",
    "\n",
    "- The directory structure of the dataset is:\n",
    "\n",
    "    ```\n",
    "    ├── small_flowers\n",
    "        ├── images\n",
    "            ├── 0\n",
    "                ├── image_1.jpg\n",
    "                ├── image_2.jpg\n",
    "                ├── ...\n",
    "            ├── 1\n",
    "                ├── image_3.jpg\n",
    "                ├── image_4.jpg\n",
    "                ├── ...\n",
    "        ├── annotations\n",
    "            ├── mapping.json\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The dataset will be downloaded from s3, and extracted into a new folder\n",
    "!wget https://s-3.s3-eu-west-1.amazonaws.com/small_flowers.zip\n",
    "!unzip small_flowers.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the folders\n",
    "path_to_images = \"./small_flowers/images\"\n",
    "path_to_annotations = \"./small_flowers/annotations\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating Annotations from Folders**\n",
    "\n",
    "To generate an annotations file from a folder of folders, the path to the root directory containing the class folders is passed to ```remo.generate_annotations_from_folders()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'./small_flowers/images/annotations.csv'"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "remo.generate_annotations_from_folders(path_to_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating Tags from Folders**\n",
    "\n",
    "To generate a tags file from a folder of folders, a dictionary mapping tag to list of paths is passed to ```remo.generate_tags_from_folders()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'train_test_valid_split.csv'"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "im_list = []\n",
    "types = (\"*.jpg\", \"*jpeg\", \"*.png\", \"*.tif\")\n",
    "for ext in types:\n",
    "    im_list.extend([os.path.basename(i) for i in glob.glob(str(path_to_images)+\"/**/\"+ext, recursive=True)])\n",
    "im_list = random.sample(im_list, len(im_list))\n",
    "\n",
    "tags_dict = {\"train\" : im_list[0:121], \"test\" : im_list[121:131], \"valid\" : im_list[131:141]}\n",
    "\n",
    "remo.generate_tags_from_folders(tags_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The JSON file is provided in the dataset, and is then converted into a mapping dictionary.\n",
    "# cat_to_index : mapping between class_index -> class_label\n",
    "mapping_json_path = os.path.join(path_to_annotations, \"mapping.json\")\n",
    "cat_to_index = dict(json.load(open(mapping_json_path)))\n",
    "mapping = { value : key for (key, value) in cat_to_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding Data to Remo**\n",
    "\n",
    "To add a dataset, you can use the ```remo.create_dataset()``` specifying the path to data and annotations. \n",
    "The class encoding is passed via a dictionary.\n",
    "\n",
    "For a complete list of formats supported please refer the <a href=\"https://remo.ai/docs/annotation-formats/\">documentation</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Acquiring data - completed                                                                           \nProcessing data - Processing images: 5/140Processing data - Processing images: 15/140Processing data - Processing images: 25/140Processing data - Processing images: 34/140Processing data - Processing images: 43/140Processing data - Processing images: 53/140Processing data - Processing images: 63/140Processing data - Processing images: 72/140Processing data - Processing images: 81/140Processing data - Processing images: 91/140Processing data - Processing images: 100/140Processing data - Processing images: 109/140Processing data - Processing images: 120/140Processing data - Processing images: 130/140Processing data - Processing annotation files: 1/1Processing data - completed                                                                          \nData upload completed\n"
    }
   ],
   "source": [
    "# The annotations.csv is generated in the same path of the sub-folder\n",
    "flowers = remo.create_dataset(name='train_test_valid_example', local_files=[path_to_images],\n",
    "                                               annotation_task = \"Image classification\",\n",
    "                                               class_encoding=cat_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Progress 100% - 1/1 - elapsed 0:00:00.001000 - speed: 1000.00 img / s, ETA: 0:00:00\nAcquiring data - completed                                                                           \nProcessing data - Processing annotation files: 1/1Processing data - completed                                                                          \nData upload completed\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'session_id': '848ccb2b-a52a-4a93-ace7-c23c3ccf7a63',\n 'created_at': '2020-07-30T14:30:29.045188Z',\n 'finished_at': '2020-07-30T14:30:38.884136Z',\n 'dataset': {'id': 52, 'name': 'train_test_valid_example'},\n 'status': 'done',\n 'substatus': '',\n 'images': {'pending': 0,\n  'total': 0,\n  'successful': 0,\n  'failed': 0,\n  'errors': [],\n  'warnings': []},\n 'annotations': {'pending': 0,\n  'total': 1,\n  'successful': 1,\n  'failed': 0,\n  'errors': [],\n  'warnings': []},\n 'errors': [],\n 'warnings': [],\n 'uploaded': {'total': {'items': 1, 'size': 3225, 'human_size': '3.1 Kb'},\n  'images': {'items': 0, 'size': 0, 'human_size': '0 b'},\n  'annotations': {'items': 1, 'size': 3225, 'human_size': '3.1 Kb'},\n  'archives': {'items': 0, 'size': 0, 'human_size': '0 b'}}}"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "annotation_id = flowers.default_annotation_set().id\n",
    "flowers.add_data(paths_to_upload=[\"./train_test_valid_split.csv\"], annotation_set_id=annotation_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing the dataset**\n",
    "\n",
    "To view your data and labels using the Remo visual interface directly in the notebook, call the ```dataset.view()``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Open http://localhost:8123/datasets/52\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n        <iframe\n            id=\"remo_frame_65ae676b-9cac-4d1d-9c68-42bc00b26c58\"\n            width=\"100%\"\n            height=\"100px\"\n            src=\"http://localhost:8123/datasets/52?allheadless\"\n            frameborder=\"0\"\n            allowfullscreen\n        ></iframe>\n        <script type=\"text/javascript\">\n            (function () {\n                const iframe = document.getElementById(\"remo_frame_65ae676b-9cac-4d1d-9c68-42bc00b26c58\");\n                let timeout, delay = 100;\n            \n                const setHeight = () => {\n                  const width = iframe.clientWidth;\n                  iframe.style.height = (width * screen.height / screen.width) * 0.8 + 'px';\n                }\n                window.addEventListener(\"resize\", () => {\n                    clearTimeout(timeout);\n                  // start timing for event \"completion\"\n                  timeout = setTimeout(setHeight, delay);\n                });\n                setHeight();\n            })()\n        </script>\n        "
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# Un-comment to view the other datasets.\n",
    "flowers.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Statistics**\n",
    "\n",
    "Remo alleviates the need to write extra boilerplate for accessing dataset properties. \n",
    "\n",
    "This can be done either using code, or via the visual interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(\"Dataset Statistics [{'AnnotationSet ID': 64, 'AnnotationSet name': 'Image \"\n \"classification', 'n_images': 0, 'n_classes': 3, 'n_objects': 0, \"\n \"'top_3_classes': [{'name': 'Hard-leaved pocket orchid', 'count': 60}, \"\n \"{'name': 'Canterbury bells', 'count': 40}, {'name': 'Pink primrose', \"\n \"'count': 40}], 'creation_date': None, 'last_modified_date': \"\n \"'2020-07-30T14:30:24.329559Z'}]\")\n"
    }
   ],
   "source": [
    "data_stats = flowers.get_annotation_statistics()\n",
    "pprint(\"Dataset Statistics {}\".format(data_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Open http://localhost:8123/annotation-detail/64/insights\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n        <iframe\n            id=\"remo_frame_b77433e8-a080-4764-9da9-d58ce6855888\"\n            width=\"100%\"\n            height=\"100px\"\n            src=\"http://localhost:8123/annotation-detail/64/insights?allheadless\"\n            frameborder=\"0\"\n            allowfullscreen\n        ></iframe>\n        <script type=\"text/javascript\">\n            (function () {\n                const iframe = document.getElementById(\"remo_frame_b77433e8-a080-4764-9da9-d58ce6855888\");\n                let timeout, delay = 100;\n            \n                const setHeight = () => {\n                  const width = iframe.clientWidth;\n                  iframe.style.height = (width * screen.height / screen.width) * 0.8 + 'px';\n                }\n                window.addEventListener(\"resize\", () => {\n                    clearTimeout(timeout);\n                  // start timing for event \"completion\"\n                  timeout = setTimeout(setHeight, delay);\n                });\n                setHeight();\n            })()\n        </script>\n        "
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# Un-comment the other lines to view the other datasets.\n",
    "flowers.view_annotation_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Export Annotations To File**\n",
    "\n",
    "Using the ```dataset.export_annotations_to_file()``` method, the annotations from Remo can be exported to a format of your choice.\n",
    "\n",
    "For a complete list of formats supported please refer the <a href=\"https://remo.ai/docs/annotation-formats/\">documentation</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Archive:  flowers_annotations.zip\n  inflating: tags.csv                \n  inflating: Image classification.csv  \n"
    }
   ],
   "source": [
    "flowers.export_annotations_to_file(\"flowers_annotations.zip\", annotation_format=\"csv\", full_path=True, export_tags=True)\n",
    "!unzip flowers_annotations.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding Data into PyTorch\n",
    "\n",
    "A custom PyTorch ```Dataset``` object defined below is used to load data.\n",
    "\n",
    "In order to adapt this to your dataset, the following are required:\n",
    "\n",
    "- **Path to data:** Path to the Data Folder\n",
    "- **Path to Annotations:** Path to Annotations CSV File (Format : file_name, class_name)\n",
    "- **Mapping:** Python dictionary containing mapping of class name and class index (Format : {\"class_name\" : \"class_index\"})\n",
    "- **transforms:** Transforms to be applied to the images before passing it to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GgrPCv6E8ipU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FlowerDataset(Dataset):\n",
    "    def __init__(self, annotations, train_test_valid_split, data_path, mapping, mode, transform=None):\n",
    "        # Pandas is used to read in the csv file into a DataFrame for data loading\n",
    "        self.data = pd.read_csv(annotations)\n",
    "        self.train_test_valid_split = pd.read_csv(train_test_valid_split)\n",
    "        self.data[\"tag\"] = self.train_test_valid_split[\"tag\"]\n",
    "        self.data_path = data_path\n",
    "        self.mapping = mapping\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        \n",
    "        if self.mode == \"train\":\n",
    "            self.data_df = self.data[self.data[\"tag\"] == \"train\"][[\"file_name\", \"classes\"]].reset_index(drop=True)\n",
    "        elif self.mode == \"test\":\n",
    "            self.data_df = self.data[self.data[\"tag\"] == \"test\"][[\"file_name\", \"classes\"]].reset_index(drop=True)\n",
    "        elif self.mode == \"valid\":\n",
    "            self.data_df = self.data[self.data[\"tag\"] == \"valid\"][[\"file_name\", \"classes\"]].reset_index(drop=True)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "            labels = int(self.mapping[self.data_df.loc[idx, 'classes'].lower()])\n",
    "            im_path = os.path.join(self.data_path,str(labels), self.data_df.loc[idx, 'file_name'])\n",
    "            \n",
    "            label_tensor = torch.as_tensor(labels-1, dtype=torch.long)\n",
    "            im = Image.open(im_path)\n",
    "        \n",
    "            if self.transform:\n",
    "                im = self.transform(im)\n",
    "        \n",
    "            if self.mode == \"test\":\n",
    "                # For saving the predicitions, the file name is required\n",
    "                return {\"im\" : im, \"labels\": label_tensor, \"im_name\" : self.data_df.loc[idx, 'file_name']}\n",
    "            else:\n",
    "                return {\"im\" : im, \"labels\" : label_tensor}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Channel wise mean and standard deviation for normalizing according to ImageNet Statistics\n",
    "means = [0.485, 0.456, 0.406]\n",
    "stds = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "# Transforms to be applied to Train-Test-Validation\n",
    "train_transforms = transforms.Compose([\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(means, stds)\n",
    "    ])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(means, stds)])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(means, stds)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train, test and validation datasets are instantiated and wrapped around a ```DataLoader``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = FlowerDataset(annotations=\"Image classification.csv\",\n",
    "                                   data_path='./flowers/',\n",
    "                                   train_test_valid_split=\"tags.csv\",\n",
    "                                   transform=train_transforms,\n",
    "                                   mapping=mapping, mode=\"train\")\n",
    "\n",
    "val_dataset = FlowerDataset(annotations=\"Image classification.csv\",\n",
    "                              data_path='./flowers',\n",
    "                               train_test_valid_split=\"tags.csv\",\n",
    "                              transform=val_transforms,\n",
    "                              mapping=mapping,\n",
    "                               mode=\"valid\")\n",
    "\n",
    "test_dataset = FlowerDataset(annotations=\"Image classification.csv\",\n",
    "                              data_path='./flowers',\n",
    "                              train_test_valid_split=\"tags.csv\",\n",
    "                              transform=test_transform,\n",
    "                              mapping=mapping,\n",
    "                              mode=\"test\")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=2,  shuffle=False, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=1, shuffle=False, num_workers=2)\n",
    "\n",
    "data_loader = {\"train\" : train_loader, \"valid\": val_loader}\n",
    "len_dict = {\"train\" : len(train_dataset), \"valid\" : len(val_dataset)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "The pre-trained weights of the ```ResNet-18``` model with ImageNet are used in this tutorial.\n",
    "\n",
    "To train the model, the following details are passed to the ```train_model()``` function\n",
    "\n",
    "1. **Model:** The edited version of the pre-trained model.\n",
    "2. **Data Loaders:** The dictionary containing our training and validation dataloaders\n",
    "3. **Criterion:** The loss function used for training the network\n",
    "4. **Num_epochs:** The number of epochs for which we would like to train the network.\n",
    "5. **dataset_size:** an additional parameter which is used to correctly scale the loss, the method for this is specified in the DataLoader cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83,
     "referenced_widgets": [
      "1e193402cc1b4dbe97531c28ac93a8bb",
      "18d04c743bb4459d8a8235e8846b178e",
      "ec07bf4e45574e9ca39217f19a0adb48",
      "822d18ace03e4637892cf40ae0996e4a",
      "ca38e79005dd4612ae97645c53113ae2",
      "2690eec5fed040cd801e83c2f178d068",
      "b37da2626af94b649d19213f38e14d65",
      "403ac43de7454dd4b4dfdab3a61ffe14"
     ]
    },
    "colab_type": "code",
    "id": "lCxV4MgZ8fEs",
    "outputId": "325236d0-57df-49f3-b881-a81183de8559"
   },
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freezing the weights\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.required_grad = False\n",
    "\n",
    "\n",
    "# Replacing the final layer\n",
    "model.fc = nn.Sequential(nn.Linear(512, 256), nn.ReLU(), nn.Dropout(p=0.5), nn.Linear(256, 102),\n",
    "                         nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_usNSOE7Ao6n"
   },
   "outputs": [],
   "source": [
    "def train_model(model, data_loaders, optimizer, criterion, num_epochs, dataset_size):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Checks if GPU is present\n",
    "\n",
    "    model.to(device) # This method pushes the model to the device.\n",
    "    \n",
    "    # The training loop trains the model for the total number of epochs,\n",
    "    # an epoch is one complete pass over the entire dataset\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train() # This sets the model back to training after the validation step\n",
    "        print(\"Epoch Number {}\".format(epoch))\n",
    "\n",
    "        training_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        val_acc = 0\n",
    "        correct_preds = 0\n",
    "        best_acc = 0\n",
    "        validation = 0.0\n",
    "        total = 0\n",
    "\n",
    "        \n",
    "        data_loader = tqdm.tqdm(data_loaders[\"train\"])\n",
    "        for x, data in enumerate(data_loader):\n",
    "            inputs, labels = data[\"im\"].to(device), data[\"labels\"].to(device)\n",
    "            outputs = model(inputs)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            training_loss += loss.item()\n",
    "        \n",
    "        epoch_loss = training_loss / dataset_size[\"train\"]\n",
    "\n",
    "        print(\"Training Loss : {:.5f}\".format(epoch_loss))\n",
    "\n",
    "        val_data_loader = tqdm.tqdm(data_loaders[\"valid\"])\n",
    "        \n",
    "        # Validation step after every epoch\n",
    "        # The gradients are not required at inference time, hence the model is set to eval mode\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for x, data in enumerate(val_data_loader):\n",
    "                inputs, labels = data[\"im\"].to(device), data[\"labels\"].to(device)\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                val_loss = criterion(outputs, labels)\n",
    "                _, index = torch.max(outputs, 1)\n",
    "\n",
    "                total += labels.size(0)\n",
    "                correct_preds += (index == labels).sum().item()\n",
    "\n",
    "                validation += val_loss.item()\n",
    "\n",
    "            val_acc = 100 * (correct_preds / total)\n",
    "\n",
    "            print(\"Validation Loss : {:.5f}\".format(validation / dataset_size[\"valid\"]))\n",
    "            print(\"Validation Accuracy is: {:.2f}%\".format(val_acc))\n",
    "            \n",
    "            # The model is saved only if current validation accuracy is higher than the previous best accuracy\n",
    "            if best_acc < val_acc:\n",
    "                best_acc = val_acc\n",
    "                model_name = \"./saved_model_{:.2f}.pt\".format(best_acc)\n",
    "                torch.save(model, model_name)\n",
    "                \n",
    "            return model_name \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sKs3wDOaDmc0",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "0%|          | 0/4 [00:00<?, ?it/s]Epoch Number 0\n100%|██████████| 4/4 [00:12<00:00,  3.09s/it]\n  0%|          | 0/5 [00:00<?, ?it/s]Training Loss : 0.11209\n100%|██████████| 5/5 [00:00<00:00,  7.77it/s]\nValidation Loss : 0.76405\nValidation Accuracy is: 88.89%\n"
    }
   ],
   "source": [
    "# We use the Adam optimizer, which inherits the parameters of only the trainable layers.\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "trained_model = train_model(model=model, data_loaders=data_loader, optimizer=optimizer, num_epochs=1, dataset_size=len_dict, criterion=nn.NLLLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oASN5m6Ibn9l"
   },
   "outputs": [],
   "source": [
    "def test_model(dataloader, model):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    test_model = torch.load(model)\n",
    "    \n",
    "    test_model.eval()\n",
    "    tk0 = tqdm.tqdm(dataloader)\n",
    "    \n",
    "    total = 0\n",
    "    correct_preds = 0\n",
    "    pred_list = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, data in enumerate(tk0):\n",
    "            single_im, label = data[\"im\"].to(device), data[\"labels\"].to(device)\n",
    "            im_name = data[\"im_name\"]\n",
    "            \n",
    "            pred = test_model(single_im)\n",
    "\n",
    "            _, index = torch.max(pred, 1)\n",
    "\n",
    "            total += label.size(0)\n",
    "            correct_preds += (index == label).sum().item()\n",
    "            \n",
    "            pred_list[os.path.basename(im_name[0])] = (index+1).item()\n",
    "            \n",
    "    df = pd.DataFrame(pred_list.items(), columns=['file_name', 'class_name'])\n",
    "    with open(\"results.csv\", \"w\") as f:\n",
    "        df.to_csv(f, index=False)\n",
    "    print('Accuracy of the network on the test images: %d %%' % (100 * (correct_preds / total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kkZFRcSncgFa",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 10/10 [00:00<00:00, 16.31it/s]Accuracy of the network on the test images: 70 %\n\n"
    }
   ],
   "source": [
    "test_model (test_loader, trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Predictions\n",
    "\n",
    "For visualizing the predicted v/s original label in Remo, the predictions are added to a CSV, which is then added as an ```AnnotationSet``` to the testing_dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['fire lily', 'canterbury bells', 'bolero deep blue', 'pink primrose', 'mexican aster', 'prince of wales feathers', 'moon orchid', 'globe-flower', 'grape hyacinth', 'corn poppy', 'toad lily', 'siam tulip', 'red ginger', 'spring crocus', 'alpine sea holly', 'garden phlox', 'globe thistle', 'tiger lily', 'ball moss', 'love in the mist', 'monkshood', 'blackberry lily', 'spear thistle', 'balloon flower', 'blanket flower', 'king protea', 'oxeye daisy', 'yellow iris', 'cautleya spicata', 'carnation', 'silverbush', 'bearded iris', 'black-eyed susan', 'windflower', 'japanese anemone', 'giant white arum lily', 'great masterwort', 'sweet pea', 'tree mallow', 'trumpet creeper', 'daffodil', 'pincushion flower', 'hard-leaved pocket orchid', 'sunflower', 'osteospermum', 'tree poppy', 'desert-rose', 'bromelia', 'magnolia', 'english marigold', 'bee balm', 'stemless gentian', 'mallow', 'gaura', 'lenten rose', 'marigold', 'orange dahlia', 'buttercup', 'pelargonium', 'ruby-lipped cattleya', 'hippeastrum', 'artichoke', 'gazania', 'canna lily', 'peruvian lily', 'mexican petunia', 'bird of paradise', 'sweet william', 'purple coneflower', 'wild pansy', 'columbine', \"colt's foot\", 'snapdragon', 'camellia', 'fritillary', 'common dandelion', 'poinsettia', 'primula', 'azalea', 'californian poppy', 'anthurium', 'morning glory', 'cape flower', 'bishop of llandaff', 'pink-yellow dahlia', 'clematis', 'geranium', 'thorn apple', 'barbeton daisy', 'bougainvillea', 'sword lily', 'hibiscus', 'lotus', 'cyclamen', 'foxglove', 'frangipani', 'rose', 'watercress', 'water lily', 'wallflower', 'passion flower', 'petunia']\n"
    }
   ],
   "source": [
    "classes = list(mapping.keys())\n",
    "print(classes)\n",
    "#annotation_set = remo.create_annotation_set(\"Image Classification\", name=\"model_predictionss\", dataset_id=flowers.id, classes=classes)\n",
    "#flowers.add_data(local_files=[\"./results.csv\"], annotation_set_id=annotation_set.id, class_encoding=cat_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flowers.view()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "remo_image_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('pt_36': conda)",
   "language": "python",
   "name": "python361064bitpt36condae3e1a72d28c3471788eb7ee1d404ab9a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "18d04c743bb4459d8a8235e8846b178e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e193402cc1b4dbe97531c28ac93a8bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ec07bf4e45574e9ca39217f19a0adb48",
       "IPY_MODEL_822d18ace03e4637892cf40ae0996e4a"
      ],
      "layout": "IPY_MODEL_18d04c743bb4459d8a8235e8846b178e"
     }
    },
    "2690eec5fed040cd801e83c2f178d068": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "403ac43de7454dd4b4dfdab3a61ffe14": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "822d18ace03e4637892cf40ae0996e4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_403ac43de7454dd4b4dfdab3a61ffe14",
      "placeholder": "​",
      "style": "IPY_MODEL_b37da2626af94b649d19213f38e14d65",
      "value": " 44.7M/44.7M [00:21&lt;00:00, 2.16MB/s]"
     }
    },
    "b37da2626af94b649d19213f38e14d65": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ca38e79005dd4612ae97645c53113ae2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "ec07bf4e45574e9ca39217f19a0adb48": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2690eec5fed040cd801e83c2f178d068",
      "max": 46827520,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ca38e79005dd4612ae97645c53113ae2",
      "value": 46827520
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}