{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification Pipeline using Remo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"remo_normal.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, Remo will be used to accelerate the process of building a transfer learning pipeline for the task of Image Classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "EI72wutnsdyO",
    "outputId": "347b3820-f40b-4707-c6c9-666680d951fc"
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import pandas as pd\n",
    "import os\n",
    "import tqdm\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import remo\n",
    "remo.set_viewer('jupyter')\n",
    "from pprint import pprint\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Data to Remo\n",
    "\n",
    "- The dataset used in this example is the <a href=\"lol\">Flowers 102 Dataset</a>.\n",
    "- Run the next cell to download the data from s3, create a new folder and extract the files required.\n",
    "\n",
    "- The directory structure of the dataset is:\n",
    "\n",
    "    ```\n",
    "    ├── flower_dataset\n",
    "        ├── train                    \n",
    "            ├── 1\n",
    "            ├── image_1.jpg\n",
    "            ├── image_2.jpg\n",
    "            ├── ...\n",
    "        ├── test                    \n",
    "            ├── 1\n",
    "            ├── image_3.jpg\n",
    "            ├── image_4.jpg\n",
    "            ├── ...\n",
    "        ├── valid                    \n",
    "            ├── 1\n",
    "            ├── image_5.jpg\n",
    "            ├── image_6.jpg\n",
    "            ├── ...\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "lating: flower_dataset/valid/30/image_03531.jpg  \n   creating: flower_dataset/valid/82/\n  inflating: flower_dataset/valid/82/image_01683.jpg  \n  inflating: flower_dataset/valid/82/image_01693.jpg  \n  inflating: flower_dataset/valid/82/image_01625.jpg  \n  inflating: flower_dataset/valid/82/image_01643.jpg  \n  inflating: flower_dataset/valid/82/image_01596.jpg  \n  inflating: flower_dataset/valid/82/image_01658.jpg  \n  inflating: flower_dataset/valid/82/image_01592.jpg  \n  inflating: flower_dataset/valid/82/image_01671.jpg  \n  inflating: flower_dataset/valid/82/image_01649.jpg  \n  inflating: flower_dataset/valid/82/image_01659.jpg  \n  inflating: flower_dataset/valid/82/image_01690.jpg  \n  inflating: flower_dataset/valid/82/image_01691.jpg  \n  inflating: flower_dataset/valid/82/image_01640.jpg  \n   creating: flower_dataset/valid/28/\n  inflating: flower_dataset/valid/28/image_05258.jpg  \n  inflating: flower_dataset/valid/28/image_05265.jpg  \n  inflating: flower_dataset/valid/28/image_05257.jpg  \n  inflating: flower_dataset/valid/28/image_05267.jpg  \n  inflating: flower_dataset/valid/28/image_05272.jpg  \n   creating: flower_dataset/valid/90/\n  inflating: flower_dataset/valid/90/image_04402.jpg  \n  inflating: flower_dataset/valid/90/image_04403.jpg  \n   creating: flower_dataset/valid/9/\n  inflating: flower_dataset/valid/9/image_06414.jpg  \n  inflating: flower_dataset/valid/9/image_06420.jpg  \n  inflating: flower_dataset/valid/9/image_06398.jpg  \n   creating: flower_dataset/valid/88/\n  inflating: flower_dataset/valid/88/image_00544.jpg  \n  inflating: flower_dataset/valid/88/image_00512.jpg  \n  inflating: flower_dataset/valid/88/image_00577.jpg  \n  inflating: flower_dataset/valid/88/image_00459.jpg  \n  inflating: flower_dataset/valid/88/image_00461.jpg  \n  inflating: flower_dataset/valid/88/image_00595.jpg  \n  inflating: flower_dataset/valid/88/image_00554.jpg  \n  inflating: flower_dataset/valid/88/image_00574.jpg  \n  inflating: flower_dataset/valid/88/image_00547.jpg  \n  inflating: flower_dataset/valid/88/image_00533.jpg  \n  inflating: flower_dataset/valid/88/image_00508.jpg  \n  inflating: flower_dataset/valid/88/image_00545.jpg\n  inflating: flower_dataset/valid/88/image_00593.jpg  \n  inflating: flower_dataset/valid/88/image_00489.jpg  \n  inflating: flower_dataset/valid/88/image_00585.jpg  \n  inflating: flower_dataset/valid/88/image_00597.jpg  \n  inflating: flower_dataset/valid/88/image_00502.jpg  \n  inflating: flower_dataset/valid/88/image_00531.jpg  \n  inflating: flower_dataset/valid/88/image_00518.jpg  \n  inflating: flower_dataset/valid/88/image_00546.jpg  \n  inflating: flower_dataset/valid/88/image_00542.jpg  \n  inflating: flower_dataset/valid/88/image_00539.jpg  \n  inflating: flower_dataset/valid/88/image_00516.jpg  \n  inflating: flower_dataset/valid/88/image_00509.jpg  \n  inflating: flower_dataset/valid/88/image_00556.jpg  \n   creating: flower_dataset/valid/16/\n  inflating: flower_dataset/valid/16/image_06671.jpg  \n  inflating: flower_dataset/valid/16/image_06691.jpg  \n   creating: flower_dataset/valid/32/\n  inflating: flower_dataset/valid/32/image_05598.jpg  \n  inflating: flower_dataset/valid/32/image_05615.jpg  \n  inflating: flower_dataset/valid/32/image_05584.jpg  \n   creating: flower_dataset/valid/66/\n  inflating: flower_dataset/valid/66/image_05563.jpg  \n  inflating: flower_dataset/valid/66/image_05577.jpg  \n  inflating: flower_dataset/valid/66/image_05559.jpg  \n  inflating: flower_dataset/valid/66/image_05560.jpg  \n  inflating: flower_dataset/valid/66/image_05579.jpg  \n  inflating: flower_dataset/valid/66/image_05565.jpg  \n   creating: flower_dataset/valid/49/\n  inflating: flower_dataset/valid/49/image_06209.jpg  \n  inflating: flower_dataset/valid/49/image_06216.jpg  \n  inflating: flower_dataset/valid/49/image_06210.jpg  \n  inflating: flower_dataset/valid/49/image_06228.jpg  \n  inflating: flower_dataset/valid/49/image_06240.jpg  \n  inflating: flower_dataset/valid/49/image_06230.jpg  \n  inflating: flower_dataset/valid/49/image_06222.jpg  \n  inflating: flower_dataset/valid/49/image_06235.jpg  \n   creating: flower_dataset/valid/23/\n  inflating: flower_dataset/valid/23/image_03425.jpg  \n  inflating: flower_dataset/valid/23/image_03436.jpg  \n  inflating: flower_dataset/valid/23/image_03420.jpg  \n  inflating: flower_dataset/valid/23/image_03400.jpg  \n  inflating: flower_dataset/valid/23/image_03383.jpg  \n  inflating: flower_dataset/valid/23/image_03398.jpg  \n  inflating: flower_dataset/valid/23/image_03386.jpg  \n  inflating: flower_dataset/valid/23/image_03416.jpg  \n  inflating: flower_dataset/valid/23/image_03444.jpg  \n  inflating: flower_dataset/valid/23/image_03437.jpg  \n  inflating: flower_dataset/valid/23/image_03371.jpg  \n  inflating: flower_dataset/valid/23/image_03412.jpg  \n   creating: flower_dataset/valid/48/\n  inflating: flower_dataset/valid/48/image_04678.jpg  \n  inflating: flower_dataset/valid/48/image_04684.jpg  \n  inflating: flower_dataset/valid/48/image_04691.jpg  \n  inflating: flower_dataset/valid/48/image_04688.jpg  \n  inflating: flower_dataset/valid/48/image_04693.jpg  \n  inflating: flower_dataset/valid/48/image_04687.jpg  \n  inflating: flower_dataset/valid/48/image_04690.jpg  \n  inflating: flower_dataset/valid/48/image_04641.jpg  \n  inflating: flower_dataset/valid/48/image_04652.jpg  \n   creating: flower_dataset/valid/57/\n  inflating: flower_dataset/valid/57/image_07255.jpg  \n  inflating: flower_dataset/valid/57/image_07247.jpg  \n  inflating: flower_dataset/valid/57/image_08145.jpg  \n  inflating: flower_dataset/valid/57/image_07239.jpg  \n  inflating: flower_dataset/valid/57/image_07254.jpg  \n  inflating: flower_dataset/valid/57/image_07262.jpg  \n   creating: flower_dataset/valid/27/\n  inflating: flower_dataset/valid/27/image_06868.jpg  \n   creating: flower_dataset/valid/37/\n  inflating: flower_dataset/valid/37/image_03822.jpg  \n  inflating: flower_dataset/valid/37/image_03820.jpg  \n  inflating: flower_dataset/valid/37/image_03765.jpg  \n  inflating: flower_dataset/valid/37/image_03813.jpg  \n  inflating: flower_dataset/valid/37/image_03768.jpg  \n  inflating: flower_dataset/valid/37/image_03785.jpg  \n  inflating: flower_dataset/valid/37/image_03756.jpg  \n  inflating: flower_dataset/valid/37/image_03810.jpg  \n   creating: flower_dataset/valid/38/\n  inflating: flower_dataset/valid/38/image_05829.jpg  \n  inflating: flower_dataset/valid/38/image_05830.jpg  \n  inflating: flower_dataset/valid/38/image_05819.jpg  \n  inflating: flower_dataset/valid/38/image_05845.jpg  \n   creating: flower_dataset/valid/44/\n  inflating: flower_dataset/valid/44/image_01569.jpg  \n  inflating: flower_dataset/valid/44/image_01573.jpg  \n  inflating: flower_dataset/valid/44/image_01500.jpg  \n  inflating: flower_dataset/valid/44/image_01579.jpg  \n  inflating: flower_dataset/valid/44/image_01491.jpg  \n  inflating: flower_dataset/valid/44/image_01570.jpg  \n  inflating: flower_dataset/valid/44/image_01540.jpg  \n  inflating: flower_dataset/valid/44/image_01494.jpg  \n  inflating: flower_dataset/valid/44/image_01510.jpg  \n   creating: flower_dataset/valid/29/\n  inflating: flower_dataset/valid/29/image_04116.jpg  \n  inflating: flower_dataset/valid/29/image_04103.jpg  \n  inflating: flower_dataset/valid/29/image_04108.jpg  \n  inflating: flower_dataset/valid/29/image_04097.jpg  \n  inflating: flower_dataset/valid/29/image_04099.jpg  \n  inflating: flower_dataset/valid/29/image_04104.jpg  \n  inflating: flower_dataset/valid/29/image_04143.jpg  \n   creating: flower_dataset/valid/55/\n  inflating: flower_dataset/valid/55/image_04734.jpg  \n  inflating: flower_dataset/valid/55/image_04739.jpg  \n  inflating: flower_dataset/valid/55/image_04702.jpg  \n  inflating: flower_dataset/valid/55/image_04722.jpg  \n  inflating: flower_dataset/valid/55/image_04696.jpg  \n  inflating: flower_dataset/valid/55/image_04753.jpg  \n  inflating: flower_dataset/valid/55/image_04760.jpg  \n  inflating: flower_dataset/valid/55/image_04720.jpg  \n   creating: flower_dataset/valid/85/\n  inflating: flower_dataset/valid/85/image_04818.jpg  \n  inflating: flower_dataset/valid/85/image_04817.jpg  \n  inflating: flower_dataset/valid/85/image_04799.jpg  \n  inflating: flower_dataset/valid/85/image_04804.jpg  \n  inflating: flower_dataset/valid/85/image_04809.jpg  \n   creating: flower_dataset/valid/10/\n  inflating: flower_dataset/valid/10/image_07101.jpg  \n  inflating: flower_dataset/valid/10/image_07107.jpg  \n  inflating: flower_dataset/valid/10/image_07094.jpg  \n  inflating: flower_dataset/valid/10/image_07102.jpg  \n   creating: flower_dataset/valid/33/\n  inflating: flower_dataset/valid/33/image_06481.jpg  \n  inflating: flower_dataset/valid/33/image_06455.jpg\n  inflating: flower_dataset/valid/33/image_06441.jpg  \n  inflating: flower_dataset/valid/33/image_06443.jpg  \n  inflating: flower_dataset/valid/33/image_06485.jpg  \n  inflating: flower_dataset/valid/33/image_06444.jpg  \n  inflating: flower_dataset/valid/33/image_06468.jpg  \n   creating: flower_dataset/valid/59/\n  inflating: flower_dataset/valid/59/image_05086.jpg  \n  inflating: flower_dataset/valid/59/image_05056.jpg  \n  inflating: flower_dataset/valid/59/image_05077.jpg  \n  inflating: flower_dataset/valid/59/image_05034.jpg  \n   creating: flower_dataset/valid/6/\n  inflating: flower_dataset/valid/6/image_08105.jpg  \n   creating: flower_dataset/valid/22/\n  inflating: flower_dataset/valid/22/image_05361.jpg  \n  inflating: flower_dataset/valid/22/image_05376.jpg  \n  inflating: flower_dataset/valid/22/image_05368.jpg  \n  inflating: flower_dataset/valid/22/image_05352.jpg  \n  inflating: flower_dataset/valid/22/image_05398.jpg  \n  inflating: flower_dataset/valid/22/image_05381.jpg  \n  inflating: flower_dataset/valid/22/image_05388.jpg  \n  inflating: flower_dataset/valid/22/image_05364.jpg  \n   creating: flower_dataset/valid/14/\n  inflating: flower_dataset/valid/14/image_06082.jpg  \n   creating: flower_dataset/valid/96/\n  inflating: flower_dataset/valid/96/image_07613.jpg  \n  inflating: flower_dataset/valid/96/image_07669.jpg  \n  inflating: flower_dataset/valid/96/image_07656.jpg  \n  inflating: flower_dataset/valid/96/image_07662.jpg  \n  inflating: flower_dataset/valid/96/image_07670.jpg  \n  inflating: flower_dataset/valid/96/image_07677.jpg  \n  inflating: flower_dataset/valid/96/image_07605.jpg  \n  inflating: flower_dataset/valid/96/image_07658.jpg  \n  inflating: flower_dataset/valid/96/image_07653.jpg  \n  inflating: flower_dataset/valid/96/image_07609.jpg  \n   creating: flower_dataset/valid/21/\n  inflating: flower_dataset/valid/21/image_06778.jpg  \n  inflating: flower_dataset/valid/21/image_06784.jpg  \n  inflating: flower_dataset/valid/21/image_06788.jpg  \n  inflating: flower_dataset/valid/21/image_06804.jpg  \n   creating: flower_dataset/valid/73/\n  inflating: flower_dataset/valid/73/image_00302.jpg  \n  inflating: flower_dataset/valid/73/image_00352.jpg  \n  inflating: flower_dataset/valid/73/image_00378.jpg  \n  inflating: flower_dataset/valid/73/image_00354.jpg  \n  inflating: flower_dataset/valid/73/image_00356.jpg  \n  inflating: flower_dataset/valid/73/image_00329.jpg  \n  inflating: flower_dataset/valid/73/image_00295.jpg  \n  inflating: flower_dataset/valid/73/image_00421.jpg  \n  inflating: flower_dataset/valid/73/image_00280.jpg  \n  inflating: flower_dataset/valid/73/image_00366.jpg  \n  inflating: flower_dataset/valid/73/image_00432.jpg  \n  inflating: flower_dataset/valid/73/image_00415.jpg  \n  inflating: flower_dataset/valid/73/image_00440.jpg  \n  inflating: flower_dataset/valid/73/image_00424.jpg  \n  inflating: flower_dataset/valid/73/image_00442.jpg  \n  inflating: flower_dataset/valid/73/image_00436.jpg  \n  inflating: flower_dataset/valid/73/image_00381.jpg  \n  inflating: flower_dataset/valid/73/image_00402.jpg  \n  inflating: flower_dataset/valid/73/image_00275.jpg  \n   creating: flower_dataset/valid/62/\n  inflating: flower_dataset/valid/62/image_07271.jpg  \n  inflating: flower_dataset/valid/62/image_08155.jpg  \n  inflating: flower_dataset/valid/62/image_08171.jpg  \n   creating: flower_dataset/valid/95/\n  inflating: flower_dataset/valid/95/image_07502.jpg  \n  inflating: flower_dataset/valid/95/image_07512.jpg  \n  inflating: flower_dataset/valid/95/image_07485.jpg  \n  inflating: flower_dataset/valid/95/image_07475.jpg  \n  inflating: flower_dataset/valid/95/image_07588.jpg  \n  inflating: flower_dataset/valid/95/image_07581.jpg  \n  inflating: flower_dataset/valid/95/image_07575.jpg  \n  inflating: flower_dataset/valid/95/image_07510.jpg  \n  inflating: flower_dataset/valid/95/image_07513.jpg  \n  inflating: flower_dataset/valid/95/image_07580.jpg  \n  inflating: flower_dataset/valid/95/image_07585.jpg  \n  inflating: flower_dataset/valid/95/image_07471.jpg  \n  inflating: flower_dataset/valid/95/image_07484.jpg  \n   creating: flower_dataset/valid/61/\n  inflating: flower_dataset/valid/61/image_06259.jpg  \n  inflating: flower_dataset/valid/61/image_06273.jpg  \n  inflating: flower_dataset/valid/61/image_06261.jpg  \n  inflating: flower_dataset/valid/61/image_06292.jpg  \n  inflating: flower_dataset/valid/61/image_06296.jpg  \n  inflating: flower_dataset/valid/61/image_06293.jpg  \n   creating: flower_dataset/valid/94/\n  inflating: flower_dataset/valid/94/image_07317.jpg  \n  inflating: flower_dataset/valid/94/image_07381.jpg  \n  inflating: flower_dataset/valid/94/image_07372.jpg  \n  inflating: flower_dataset/valid/94/image_07385.jpg  \n  inflating: flower_dataset/valid/94/image_07325.jpg  \n  inflating: flower_dataset/valid/94/image_07360.jpg  \n  inflating: flower_dataset/valid/94/image_07408.jpg  \n  inflating: flower_dataset/valid/94/image_07435.jpg  \n  inflating: flower_dataset/valid/94/image_07393.jpg  \n  inflating: flower_dataset/valid/94/image_07428.jpg  \n  inflating: flower_dataset/valid/94/image_07418.jpg  \n  inflating: flower_dataset/valid/94/image_07449.jpg  \n  inflating: flower_dataset/valid/94/image_07382.jpg  \n  inflating: flower_dataset/valid/94/image_07392.jpg  \n   creating: flower_dataset/valid/93/\n  inflating: flower_dataset/valid/93/image_06048.jpg  \n  inflating: flower_dataset/valid/93/image_06024.jpg  \n  inflating: flower_dataset/valid/93/image_07303.jpg  \n  inflating: flower_dataset/valid/93/image_06044.jpg  \n  inflating: flower_dataset/valid/93/image_07302.jpg  \n  inflating: flower_dataset/valid/93/image_08113.jpg  \n   creating: flower_dataset/valid/86/\n  inflating: flower_dataset/valid/86/image_02909.jpg  \n  inflating: flower_dataset/valid/86/image_02887.jpg  \n  inflating: flower_dataset/valid/86/image_02906.jpg  \n  inflating: flower_dataset/valid/86/image_02901.jpg  \n  inflating: flower_dataset/valid/86/image_02891.jpg  \n   creating: flower_dataset/valid/89/\n  inflating: flower_dataset/valid/89/image_00623.jpg  \n  inflating: flower_dataset/valid/89/image_00752.jpg  \n  inflating: flower_dataset/valid/89/image_00624.jpg  \n  inflating: flower_dataset/valid/89/image_00691.jpg  \n  inflating: flower_dataset/valid/89/image_00730.jpg  \n  inflating: flower_dataset/valid/89/image_00764.jpg  \n  inflating: flower_dataset/valid/89/image_00749.jpg  \n  inflating: flower_dataset/valid/89/image_00746.jpg  \n  inflating: flower_dataset/valid/89/image_00759.jpg\n  inflating: flower_dataset/valid/89/image_00741.jpg  \n  inflating: flower_dataset/valid/89/image_00696.jpg  \n  inflating: flower_dataset/valid/89/image_00692.jpg  \n  inflating: flower_dataset/valid/89/image_00627.jpg  \n  inflating: flower_dataset/valid/89/image_00763.jpg  \n  inflating: flower_dataset/valid/89/image_00652.jpg  \n  inflating: flower_dataset/valid/89/image_00640.jpg  \n   creating: flower_dataset/valid/25/\n  inflating: flower_dataset/valid/25/image_06584.jpg  \n  inflating: flower_dataset/valid/25/image_06572.jpg  \n   creating: flower_dataset/valid/69/\n  inflating: flower_dataset/valid/69/image_05984.jpg  \n  inflating: flower_dataset/valid/69/image_05992.jpg  \n  inflating: flower_dataset/valid/69/image_06008.jpg  \n  inflating: flower_dataset/valid/69/image_05958.jpg  \n  inflating: flower_dataset/valid/69/image_05993.jpg  \n   creating: flower_dataset/valid/40/\n  inflating: flower_dataset/valid/40/image_04578.jpg  \n  inflating: flower_dataset/valid/40/image_04596.jpg  \n  inflating: flower_dataset/valid/40/image_04608.jpg  \n  inflating: flower_dataset/valid/40/image_04579.jpg  \n  inflating: flower_dataset/valid/40/image_04584.jpg  \n   creating: flower_dataset/valid/1/\n  inflating: flower_dataset/valid/1/image_06763.jpg  \n  inflating: flower_dataset/valid/1/image_06765.jpg  \n  inflating: flower_dataset/valid/1/image_06739.jpg  \n  inflating: flower_dataset/valid/1/image_06756.jpg  \n  inflating: flower_dataset/valid/1/image_06755.jpg  \n  inflating: flower_dataset/valid/1/image_06749.jpg  \n  inflating: flower_dataset/valid/1/image_06769.jpg  \n  inflating: flower_dataset/valid/1/image_06758.jpg  \n   creating: flower_dataset/valid/41/\n  inflating: flower_dataset/valid/41/image_02198.jpg  \n  inflating: flower_dataset/valid/41/image_02260.jpg  \n  inflating: flower_dataset/valid/41/image_02201.jpg  \n  inflating: flower_dataset/valid/41/image_02205.jpg  \n  inflating: flower_dataset/valid/41/image_02276.jpg  \n  inflating: flower_dataset/valid/41/image_02231.jpg  \n  inflating: flower_dataset/valid/41/image_02268.jpg  \n  inflating: flower_dataset/valid/41/image_02248.jpg  \n  inflating: flower_dataset/valid/41/image_02232.jpg  \n  inflating: flower_dataset/valid/41/image_02199.jpg  \n  inflating: flower_dataset/valid/41/image_02247.jpg  \n  inflating: flower_dataset/valid/41/image_02278.jpg  \n  inflating: flower_dataset/valid/41/image_02258.jpg  \n  inflating: flower_dataset/valid/41/image_02219.jpg  \n  inflating: flower_dataset/valid/41/image_02297.jpg  \n  inflating: flower_dataset/valid/41/image_02259.jpg  \n   creating: flower_dataset/valid/11/\n  inflating: flower_dataset/valid/11/image_03157.jpg  \n  inflating: flower_dataset/valid/11/image_03173.jpg  \n  inflating: flower_dataset/valid/11/image_03111.jpg  \n  inflating: flower_dataset/valid/11/image_03145.jpg  \n  inflating: flower_dataset/valid/11/image_03116.jpg  \n  inflating: flower_dataset/valid/11/image_03128.jpg  \n  inflating: flower_dataset/valid/11/image_03100.jpg  \n  inflating: flower_dataset/valid/11/image_03125.jpg  \n  inflating: flower_dataset/valid/11/image_03139.jpg  \n  inflating: flower_dataset/valid/11/image_03108.jpg  \n   creating: flower_dataset/valid/3/\n  inflating: flower_dataset/valid/3/image_06631.jpg  \n  inflating: flower_dataset/valid/3/image_06621.jpg  \n   creating: flower_dataset/valid/8/\n  inflating: flower_dataset/valid/8/image_03366.jpg  \n  inflating: flower_dataset/valid/8/image_03349.jpg  \n  inflating: flower_dataset/valid/8/image_03313.jpg  \n  inflating: flower_dataset/valid/8/image_03330.jpg  \n  inflating: flower_dataset/valid/8/image_03342.jpg  \n   creating: flower_dataset/valid/67/\n  inflating: flower_dataset/valid/67/image_07075.jpg  \n  inflating: flower_dataset/valid/67/image_07083.jpg  \n   creating: flower_dataset/valid/5/\n  inflating: flower_dataset/valid/5/image_05164.jpg  \n  inflating: flower_dataset/valid/5/image_05196.jpg  \n  inflating: flower_dataset/valid/5/image_05168.jpg  \n  inflating: flower_dataset/valid/5/image_05199.jpg  \n  inflating: flower_dataset/valid/5/image_05192.jpg  \n  inflating: flower_dataset/valid/5/image_05209.jpg  \n  inflating: flower_dataset/valid/5/image_05188.jpg  \n   creating: flower_dataset/valid/79/\n  inflating: flower_dataset/valid/79/image_06707.jpg  \n  inflating: flower_dataset/valid/79/image_06728.jpg  \n  inflating: flower_dataset/valid/79/image_06718.jpg  \n  inflating: flower_dataset/valid/79/image_06695.jpg  \n   creating: flower_dataset/valid/20/\n  inflating: flower_dataset/valid/20/image_04913.jpg  \n  inflating: flower_dataset/valid/20/image_04918.jpg  \n  inflating: flower_dataset/valid/20/image_04942.jpg  \n  inflating: flower_dataset/valid/20/image_04940.jpg  \n  inflating: flower_dataset/valid/20/image_04920.jpg  \n  inflating: flower_dataset/valid/20/image_04903.jpg  \n  inflating: flower_dataset/valid/20/image_04927.jpg  \n  inflating: flower_dataset/valid/annotations.csv  \n"
    }
   ],
   "source": [
    "# The dataset will be downloaded from s3, and extracted into a new folder\n",
    "!wget https://s-3.s3-eu-west-1.amazonaws.com/flower_dataset.zip\n",
    "!unzip flower_dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the folders\n",
    "train_path = \"flower_dataset/train\"\n",
    "valid_path = \"flower_dataset/valid\"\n",
    "test_path = \"flower_dataset/test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating Annotations from Folders**\n",
    "\n",
    "To generate an annotations file from a folder of folders, the path to the root directory containing the class folders is passed to ```remo.generate_annotations_from_folders()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'flower_dataset/valid/annotations.csv'"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "remo.generate_annotations_from_folders(train_path)\n",
    "remo.generate_annotations_from_folders(test_path)\n",
    "remo.generate_annotations_from_folders(valid_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The JSON file is provided in the dataset, and is then converted into a mapping dictionary.\n",
    "# cat_to_index : mapping between class_index -> class_label\n",
    "\n",
    "cat_to_index = dict(json.load(open(\"flower_dataset/cat_to_name.json\")))\n",
    "mapping = { value : key for (key, value) in cat_to_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding Data to Remo**\n",
    "\n",
    "To add a dataset, you can use the ```remo.create_dataset()``` specifying the path to data and annotations. \n",
    "The class encoding is passed via a dictionary.\n",
    "\n",
    "For a complete list of formats supported please refer the <a href=\"https://remo.ai/docs/annotation-formats/\">documentation</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ges: 2394/6552Processing data - Processing images: 2410/6552Processing data - Processing images: 2416/6552Processing data - Processing images: 2429/6552Processing data - Processing images: 2447/6552Processing data - Processing images: 2461/6552Processing data - Processing images: 2471/6552Processing data - Processing images: 2485/6552Processing data - Processing images: 2500/6552Processing data - Processing images: 2511/6552Processing data - Processing images: 2526/6552Processing data - Processing images: 2546/6552Processing data - Processing images: 2559/6552Processing data - Processing images: 2571/6552Processing data - Processing images: 2584/6552Processing data - Processing images: 2595/6552Processing data - Processing images: 2609/6552Processing data - Processing images: 2625/6552Processing data - Processing images: 2638/6552Processing data - Processing images: 2652/6552Processing data - Processing images: 2667/6552Processing data - Processing images: 2684/6552Processing data - Processing images: 2697/6552Processing data - Processing images: 2712/6552Processing data - Processing images: 2729/6552Processing data - Processing images: 2747/6552Processing data - Processing images: 2761/6552Processing data - Processing images: 2771/6552Processing data - Processing images: 2783/6552Processing data - Processing images: 2798/6552Processing data - Processing images: 2812/6552Processing data - Processing images: 2823/6552Processing data - Processing images: 2837/6552Processing data - Processing images: 2850/6552Processing data - Processing images: 2864/6552Processing data - Processing images: 2876/6552Processing data - Processing images: 2892/6552Processing data - Processing images: 2906/6552Processing data - Processing images: 2919/6552Processing data - Processing images: 2930/6552Processing data - Processing images: 2942/6552Processing data - Processing images: 2955/6552Processing data - Processing images: 2971/6552Processing data - Processing images: 2986/6552Processing data - Processing images: 3002/6552Processing data - Processing images: 3012/6552Processing data - Processing images: 3026/6552Processing data - Processing images: 3035/6552Processing data - Processing images: 3045/6552Processing data - Processing images: 3066/6552Processing data - Processing images: 3078/6552Processing data - Processing images: 3091/6552Processing data - Processing images: 3099/6552Processing data - Processing images: 3115/6552Processing data - Processing images: 3123/6552Processing data - Processing images: 3137/6552Processing data - Processing images: 3146/6552Processing data - Processing images: 3156/6552Processing data - Processing images: 3168/6552Processing data - Processing images: 3180/6552Processing data - Processing images: 3192/6552Processing data - Processing images: 3212/6552Processing data - Processing images: 3225/6552Processing data - Processing images: 3235/6552Processing data - Processing images: 3243/6552Processing data - Processing images: 3254/6552Processing data - Processing images: 3265/6552Processing data - Processing images: 3276/6552Processing data - Processing images: 3295/6552Processing data - Processing images: 3311/6552Processing data - Processing images: 3324/6552Processing data - Processing images: 3336/6552Processing data - Processing images: 3352/6552Processing data - Processing images: 3361/6552Processing data - Processing images: 3377/6552Processing data - Processing images: 3388/6552Processing data - Processing images: 3404/6552Processing data - Processing images: 3413/6552Processing data - Processing images: 3427/6552Processing data - Processing images: 3442/6552Processing data - Processing images: 3458/6552Processing data - Processing images: 3471/6552Processing data - Processing images: 3486/6552Processing data - Processing images: 3501/6552Processing data - Processing images: 3515/6552Processing data - Processing images: 3524/6552Processing data - Processing images: 3541/6552Processing data - Processing images: 3554/6552Processing data - Processing images: 3564/6552Processing data - Processing images: 3582/6552Processing data - Processing images: 3588/6552Processing data - Processing images: 3602/6552Processing data - Processing images: 3616/6552Processing data - Processing images: 3630/6552Processing data - Processing images: 3638/6552Processing data - Processing images: 3654/6552Processing data - Processing images: 3666/6552Processing data - Processing images: 3679/6552Processing data - Processing images: 3690/6552Processing data - Processing images: 3701/6552Processing data - Processing images: 3717/6552Processing data - Processing images: 3733/6552Processing data - Processing images: 3743/6552Processing data - Processing images: 3750/6552Processing data - Processing images: 3771/6552Processing data - Processing images: 3784/6552Processing data - Processing images: 3801/6552Processing data - Processing images: 3805/6552Processing data - Processing images: 3817/6552Processing data - Processing images: 3830/6552Processing data - Processing images: 3841/6552Processing data - Processing images: 3847/6552Processing data - Processing images: 3857/6552Processing data - Processing images: 3869/6552Processing data - Processing images: 3882/6552Processing data - Processing images: 3888/6552Processing data - Processing images: 3893/6552Processing data - Processing images: 3901/6552Processing data - Processing images: 3910/6552Processing data - Processing images: 3921/6552Processing data - Processing images: 3933/6552Processing data - Processing images: 3945/6552Processing data - Processing images: 3954/6552Processing data - Processing images: 3962/6552Processing data - Processing images: 3974/6552Processing data - Processing images: 3988/6552Processing data - Processing images: 4006/6552Processing data - Processing images: 4020/6552Processing data - Processing images: 4032/6552Processing data - Processing images: 4047/6552Processing data - Processing images: 4065/6552Processing data - Processing images: 4072/6552Processing data - Processing images: 4078/6552Processing data - Processing images: 4092/6552Processing data - Processing images: 4107/6552Processing data - Processing images: 4121/6552Processing data - Processing images: 4131/6552Processing data - Processing images: 4144/6552Processing data - Processing images: 4158/6552Processing data - Processing images: 4172/6552Processing data - Processing images: 4186/6552Processing data - Processing images: 4202/6552Processing data - Processing images: 4219/6552Processing data - Processing images: 4236/6552Processing data - Processing images: 4247/6552Processing data - Processing images: 4254/6552Processing data - Processing images: 4270/6552Processing data - Processing images: 4287/6552Processing data - Processing images: 4305/6552Processing data - Processing images: 4315/6552Processing data - Processing images: 4331/6552Processing data - Processing images: 4348/6552Processing data - Processing images: 4362/6552Processing data - Processing images: 4379/6552Processing data - Processing images: 4392/6552Processing data - Processing images: 4407/6552Processing data - Processing images: 4422/6552Processing data - Processing images: 4430/6552Processing data - Processing images: 4447/6552Processing data - Processing images: 4466/6552Processing data - Processing images: 4484/6552Processing data - Processing images: 4498/6552Processing data - Processing images: 4510/6552Processing data - Processing images: 4524/6552Processing data - Processing images: 4540/6552Processing data - Processing images: 4558/6552Processing data - Processing images: 4574/6552Processing data - Processing images: 4588/6552Processing data - Processing images: 4598/6552Processing data - Processing images: 4615/6552Processing data - Processing images: 4625/6552Processing data - Processing images: 4640/6552Processing data - Processing images: 4657/6552Processing data - Processing images: 4670/6552Processing data - Processing images: 4678/6552Processing data - Processing images: 4685/6552Processing data - Processing images: 4698/6552Processing data - Processing images: 4710/6552Processing data - Processing images: 4725/6552Processing data - Processing images: 4735/6552Processing data - Processing images: 4750/6552Processing data - Processing images: 4768/6552Processing data - Processing images: 4783/6552Processing data - Processing images: 4793/6552Processing data - Processing images: 4807/6552Processing data - Processing images: 4822/6552Processing data - Processing images: 4834/6552Processing data - Processing images: 4849/6552Processing data - Processing images: 4864/6552Processing data - Processing images: 4875/6552Processing data - Processing images: 4890/6552Processing data - Processing images: 4904/6552Processing data - Processing images: 4919/6552Processing data - Processing images: 4933/6552Processing data - Processing images: 4945/6552Processing data - Processing images: 4961/6552Processing data - Processing images: 4972/6552Processing data - Processing images: 4985/6552Processing data - Processing images: 4995/6552Processing data - Processing images: 5010/6552Processing data - Processing images: 5020/6552Processing data - Processing images: 5035/6552Processing data - Processing images: 5046/6552Processing data - Processing images: 5054/6552Processing data - Processing images: 5062/6552Processing data - Processing images: 5072/6552Processing data - Processing images: 5088/6552Processing data - Processing images: 5102/6552Processing data - Processing images: 5115/6552Processing data - Processing images: 5128/6552Processing data - Processing images: 5140/6552Processing data - Processing images: 5155/6552Processing data - Processing images: 5166/6552Processing data - Processing images: 5182/6552Processing data - Processing images: 5196/6552Processing data - Processing images: 5210/6552Processing data - Processing images: 5221/6552Processing data - Processing images: 5235/6552Processing data - Processing images: 5249/6552Processing data - Processing images: 5264/6552Processing data - Processing images: 5280/6552Processing data - Processing images: 5296/6552Processing data - Processing images: 5312/6552Processing data - Processing images: 5323/6552Processing data - Processing images: 5337/6552Processing data - Processing images: 5351/6552Processing data - Processing images: 5364/6552Processing data - Processing images: 5381/6552Processing data - Processing images: 5395/6552Processing data - Processing images: 5407/6552Processing data - Processing images: 5418/6552Processing data - Processing images: 5436/6552Processing data - Processing images: 5450/6552Processing data - Processing images: 5463/6552Processing data - Processing images: 5477/6552Processing data - Processing images: 5495/6552Processing data - Processing images: 5509/6552Processing data - Processing images: 5523/6552Processing data - Processing images: 5538/6552Processing data - Processing images: 5547/6552Processing data - Processing images: 5562/6552Processing data - Processing images: 5578/6552Processing data - Processing images: 5585/6552Processing data - Processing images: 5603/6552Processing data - Processing images: 5618/6552Processing data - Processing images: 5631/6552Processing data - Processing images: 5640/6552Processing data - Processing images: 5654/6552Processing data - Processing images: 5664/6552Processing data - Processing images: 5678/6552Processing data - Processing images: 5690/6552Processing data - Processing images: 5706/6552Processing data - Processing images: 5717/6552Processing data - Processing images: 5732/6552Processing data - Processing images: 5745/6552Processing data - Processing images: 5758/6552Processing data - Processing images: 5772/6552Processing data - Processing images: 5786/6552Processing data - Processing images: 5800/6552Processing data - Processing images: 5808/6552Processing data - Processing images: 5821/6552Processing data - Processing images: 5831/6552Processing data - Processing images: 5838/6552Processing data - Processing images: 5848/6552Processing data - Processing images: 5859/6552Processing data - Processing images: 5873/6552Processing data - Processing images: 5885/6552Processing data - Processing images: 5896/6552Processing data - Processing images: 5911/6552Processing data - Processing images: 5923/6552Processing data - Processing images: 5940/6552Processing data - Processing images: 5950/6552Processing data - Processing images: 5960/6552Processing data - Processing images: 5973/6552Processing data - Processing images: 5988/6552Processing data - Processing images: 5999/6552Processing data - Processing images: 6017/6552Processing data - Processing images: 6028/6552Processing data - Processing images: 6039/6552Processing data - Processing images: 6053/6552Processing data - Processing images: 6063/6552Processing data - Processing images: 6077/6552Processing data - Processing images: 6093/6552Processing data - Processing images: 6106/6552Processing data - Processing images: 6121/6552Processing data - Processing images: 6135/6552Processing data - Processing images: 6150/6552Processing data - Processing images: 6165/6552Processing data - Processing images: 6175/6552Processing data - Processing images: 6187/6552Processing data - Processing images: 6203/6552Processing data - Processing images: 6215/6552Processing data - Processing images: 6225/6552Processing data - Processing images: 6240/6552Processing data - Processing images: 6252/6552Processing data - Processing images: 6263/6552Processing data - Processing images: 6277/6552Processing data - Processing images: 6288/6552Processing data - Processing images: 6301/6552Processing data - Processing images: 6316/6552Processing data - Processing images: 6327/6552Processing data - Processing images: 6339/6552Processing data - Processing images: 6356/6552Processing data - Processing images: 6368/6552Processing data - Processing images: 6385/6552Processing data - Processing images: 6401/6552Processing data - Processing images: 6417/6552Processing data - Processing images: 6434/6552Processing data - Processing images: 6443/6552Processing data - Processing images: 6456/6552Processing data - Processing images: 6469/6552Processing data - Processing images: 6482/6552Processing data - Processing images: 6491/6552Processing data - Processing images: 6503/6552Processing data - Processing images: 6517/6552Processing data - Processing images: 6529/6552Processing data - Processing images: 6542/6552Processing data - Processing annotation files: 1/1Processing data - completed                                                                          \nData upload completed with some errors:\nannotations.csv: Annotations without matching image. Annotation file 'annotations.csv' has annotations for 3 images, which can't be found in the dataset:\n\t * image_07885.jpg\n\t * image_07309.jpg\n\t * image_07323.jpg\n\nProgress 24% - 203/819 - elapsed 0:00:00.001000 - speed: 203000.00 img / s, ETA: 0:00:00.003034\nProgress 48% - 399/819 - elapsed 0:00:00.001000 - speed: 399000.00 img / s, ETA: 0:00:00.001053\nProgress 72% - 595/819 - elapsed 0:00:00.001000 - speed: 595000.00 img / s, ETA: 0:00:00.000376\nProgress 97% - 797/819 - elapsed 0:00:01.001000 - speed: 796.20 img / s, ETA: 0:00:00.027631\nProgress 100% - 819/819 - elapsed 0:00:01.001000 - speed: 818.18 img / s, ETA: 0:00:00\nAcquiring data - completed                                                                           \nProcessing data - Processing images: 5/818Processing data - Processing images: 16/818Processing data - Processing images: 24/818Processing data - Processing images: 29/818Processing data - Processing images: 38/818Processing data - Processing images: 45/818Processing data - Processing images: 52/818Processing data - Processing images: 59/818Processing data - Processing images: 68/818Processing data - Processing images: 77/818Processing data - Processing images: 88/818Processing data - Processing images: 98/818Processing data - Processing images: 106/818Processing data - Processing images: 115/818Processing data - Processing images: 123/818Processing data - Processing images: 132/818Processing data - Processing images: 140/818Processing data - Processing images: 149/818Processing data - Processing images: 159/818Processing data - Processing images: 167/818Processing data - Processing images: 176/818Processing data - Processing images: 186/818Processing data - Processing images: 194/818Processing data - Processing images: 202/818Processing data - Processing images: 210/818Processing data - Processing images: 213/818Processing data - Processing images: 222/818Processing data - Processing images: 231/818Processing data - Processing images: 237/818Processing data - Processing images: 246/818Processing data - Processing images: 256/818Processing data - Processing images: 265/818Processing data - Processing images: 271/818Processing data - Processing images: 280/818Processing data - Processing images: 288/818Processing data - Processing images: 299/818Processing data - Processing images: 308/818Processing data - Processing images: 317/818Processing data - Processing images: 325/818Processing data - Processing images: 334/818Processing data - Processing images: 343/818Processing data - Processing images: 349/818Processing data - Processing images: 357/818Processing data - Processing images: 366/818Processing data - Processing images: 375/818Processing data - Processing images: 383/818Processing data - Processing images: 392/818Processing data - Processing images: 401/818Processing data - Processing images: 410/818Processing data - Processing images: 415/818Processing data - Processing images: 423/818Processing data - Processing images: 432/818Processing data - Processing images: 441/818Processing data - Processing images: 448/818Processing data - Processing images: 455/818Processing data - Processing images: 465/818Processing data - Processing images: 474/818Processing data - Processing images: 482/818Processing data - Processing images: 490/818Processing data - Processing images: 498/818Processing data - Processing images: 506/818Processing data - Processing images: 515/818Processing data - Processing images: 520/818Processing data - Processing images: 529/818Processing data - Processing images: 537/818Processing data - Processing images: 546/818Processing data - Processing images: 551/818Processing data - Processing images: 559/818Processing data - Processing images: 568/818Processing data - Processing images: 575/818Processing data - Processing images: 582/818Processing data - Processing images: 592/818Processing data - Processing images: 600/818Processing data - Processing images: 607/818Processing data - Processing images: 613/818Processing data - Processing images: 622/818Processing data - Processing images: 632/818Processing data - Processing images: 641/818Processing data - Processing images: 650/818Processing data - Processing images: 658/818Processing data - Processing images: 666/818Processing data - Processing images: 674/818Processing data - Processing images: 684/818Processing data - Processing images: 692/818Processing data - Processing images: 702/818Processing data - Processing images: 710/818Processing data - Processing images: 717/818Processing data - Processing images: 727/818Processing data - Processing images: 735/818Processing data - Processing images: 745/818Processing data - Processing images: 754/818Processing data - Processing images: 760/818Processing data - Processing images: 769/818Processing data - Processing images: 778/818Processing data - Processing images: 787/818Processing data - Processing images: 794/818Processing data - Processing images: 804/818Processing data - Processing images: 813/818Processing data - Processing annotation files: 1/1Processing data - completed                                                                          \nData upload completed\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\ntesting_dataset = remo.create_dataset(name = \\'flowers-test\\',\\n                    paths_to_upload=[test_path, os.path.join(test_path, \"annotations.csv\")],\\n                    annotation_task = \"Image classification\",\\n                    class_encoding=cat_to_index)\\n'"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# The annotations.csv is generated in the same path of the sub-folder\n",
    "training_dataset = remo.create_dataset(name = 'flowers-train',\n",
    "                    paths_to_upload=[train_path, os.path.join(train_path, \"annotations.csv\")],\n",
    "                    annotation_task = \"Image classification\",\n",
    "                    class_encoding=cat_to_index)\n",
    "\n",
    "valid_dataset = remo.create_dataset(name = 'flowers-valid',\n",
    "                    paths_to_upload=[valid_path, os.path.join(valid_path, \"annotations.csv\")],\n",
    "                    annotation_task = \"Image classification\",\n",
    "                    class_encoding=cat_to_index)\n",
    "\n",
    "testing_dataset = remo.create_dataset(name = 'flowers-test',\n",
    "                    paths_to_upload=[test_path, os.path.join(test_path, \"annotations.csv\")],\n",
    "                    annotation_task = \"Image classification\",\n",
    "                    class_encoding=cat_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[Dataset 38 - 'flowers-test',\n Dataset 39 - 'flowers-train',\n Dataset 40 - 'flowers-valid']"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# Method for the getting the IDs of the created datasets\n",
    "remo.list_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing the dataset**\n",
    "\n",
    "To view your data and labels using the Remo visual interface directly in the notebook, call the ```dataset.view()``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Open http://localhost:8123/datasets/39\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n        <iframe\n            id=\"remo_frame_61d90470-11bc-4f4c-8bfe-70965c9d7d18\"\n            width=\"100%\"\n            height=\"100px\"\n            src=\"http://localhost:8123/datasets/39?allheadless\"\n            frameborder=\"0\"\n            allowfullscreen\n        ></iframe>\n        <script type=\"text/javascript\">\n            (function () {\n                const iframe = document.getElementById(\"remo_frame_61d90470-11bc-4f4c-8bfe-70965c9d7d18\");\n                let timeout, delay = 100;\n            \n                const setHeight = () => {\n                  const width = iframe.clientWidth;\n                  iframe.style.height = (width * screen.height / screen.width) * 0.8 + 'px';\n                }\n                window.addEventListener(\"resize\", () => {\n                    clearTimeout(timeout);\n                  // start timing for event \"completion\"\n                  timeout = setTimeout(setHeight, delay);\n                });\n                setHeight();\n            })()\n        </script>\n        "
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# Un-comment to view the other datasets.\n",
    "training_dataset.view()\n",
    "#validation_dataset.view()\n",
    "#testing_dataset.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Statistics**\n",
    "\n",
    "Remo alleviates the need to write extra boilerplate for accessing dataset properties. \n",
    "\n",
    "This can be done either using code, or via the visual interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(\"Training Statistics [{'AnnotationSet ID': 35, 'AnnotationSet name': 'Image \"\n \"classification', 'n_images': 0, 'n_classes': 102, 'n_objects': 0, \"\n \"'top_3_classes': [{'name': 'Petunia', 'count': 206}, {'name': 'Passion \"\n \"flower', 'count': 205}, {'name': 'Wallflower', 'count': 157}], \"\n \"'creation_date': None, 'last_modified_date': '2020-07-22T11:12:56.197670Z'}]\")\n(\"Validation Statistics [{'AnnotationSet ID': 36, 'AnnotationSet name': 'Image \"\n \"classification', 'n_images': 0, 'n_classes': 102, 'n_objects': 0, \"\n \"'top_3_classes': [{'name': 'Petunia', 'count': 28}, {'name': 'Cyclamen', \"\n \"'count': 25}, {'name': 'Passion flower', 'count': 21}], 'creation_date': \"\n \"None, 'last_modified_date': '2020-07-22T11:18:08.274564Z'}]\")\n(\"Testing Statistics [{'AnnotationSet ID': 34, 'AnnotationSet name': 'Image \"\n \"classification', 'n_images': 0, 'n_classes': 102, 'n_objects': 0, \"\n \"'top_3_classes': [{'name': 'Water lily', 'count': 28}, {'name': 'Passion \"\n \"flower', 'count': 25}, {'name': 'Petunia', 'count': 24}], 'creation_date': \"\n \"None, 'last_modified_date': '2020-07-22T10:56:51.998444Z'}]\")\n"
    }
   ],
   "source": [
    "train_stats = training_dataset.get_annotation_statistics()\n",
    "valid_stats = valid_dataset.get_annotation_statistics()\n",
    "test_stats = testing_dataset.get_annotation_statistics()\n",
    "\n",
    "pprint(\"Training Statistics {}\".format(train_stats))\n",
    "pprint(\"Validation Statistics {}\".format(valid_stats))\n",
    "pprint(\"Testing Statistics {}\".format(test_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Open http://localhost:8123/annotation-detail/35/insights\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n        <iframe\n            id=\"remo_frame_a94707a8-374f-4bfe-a9f3-ac64a20574dd\"\n            width=\"100%\"\n            height=\"100px\"\n            src=\"http://localhost:8123/annotation-detail/35/insights?allheadless\"\n            frameborder=\"0\"\n            allowfullscreen\n        ></iframe>\n        <script type=\"text/javascript\">\n            (function () {\n                const iframe = document.getElementById(\"remo_frame_a94707a8-374f-4bfe-a9f3-ac64a20574dd\");\n                let timeout, delay = 100;\n            \n                const setHeight = () => {\n                  const width = iframe.clientWidth;\n                  iframe.style.height = (width * screen.height / screen.width) * 0.8 + 'px';\n                }\n                window.addEventListener(\"resize\", () => {\n                    clearTimeout(timeout);\n                  // start timing for event \"completion\"\n                  timeout = setTimeout(setHeight, delay);\n                });\n                setHeight();\n            })()\n        </script>\n        "
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "# Un-comment the other lines to view the other datasets.\n",
    "\n",
    "training_dataset.view_annotation_stats()\n",
    "#validation_dataset.view_annotation_stats()\n",
    "#testing_dataset.view_annotation_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Export Annotations To File**\n",
    "\n",
    "Using the ```dataset.export_annotations_to_file()``` method, the annotations from Remo can be exported to a format of your choice.\n",
    "\n",
    "For a complete list of formats supported please refer the <a href=\"https://remo.ai/docs/annotation-formats/\">documentation</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset.export_annotations_to_file(\"training.csv\", annotation_format=\"csv\", full_path='true')\n",
    "testing_dataset.export_annotations_to_file(\"testing.csv\", annotation_format=\"csv\", full_path='true')\n",
    "valid_dataset.export_annotations_to_file(\"validation.csv\", annotation_format=\"csv\", full_path='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding Data into PyTorch\n",
    "\n",
    "A custom PyTorch ```Dataset``` object defined below is used to load data.\n",
    "\n",
    "In order to adapt this to your dataset, the following are required:\n",
    "\n",
    "- **Path to data:** Path to the Data Folder\n",
    "- **Path to Annotations:** Path to Annotations CSV File (Format : file_name, class_name)\n",
    "- **Mapping:** Python dictionary containing mapping of class name and class index (Format : {\"class_name\" : \"class_index\"})\n",
    "- **transforms:** Transforms to be applied to the images before passing it to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GgrPCv6E8ipU"
   },
   "outputs": [],
   "source": [
    "class FlowerDataset(Dataset):\n",
    "    def __init__(self, annotations, data_path, mapping, transform=None, mode=\"train\"):\n",
    "        \n",
    "        # Pandas is used to read in the csv file into a DataFrame for data loading\n",
    "        self.data = pd.read_csv(annotations)\n",
    "        self.data_path = data_path\n",
    "        self.mapping = mapping\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        labels = int(self.mapping[self.data.loc[idx, 'classes'].lower()])\n",
    "        im_path = self.data_path + \"/\" + str(labels) + \"/\" + self.data.loc[idx, 'file_name']\n",
    "        label_tensor = torch.as_tensor(labels-1, dtype=torch.long)\n",
    "        im = Image.open(im_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            im = self.transform(im)\n",
    "        if self.mode == \"test\":\n",
    "            # For saving the predicitions, the file name is required\n",
    "            return {\"im\" : im, \"labels\": label_tensor, \"im_name\" : self.data.loc[idx, 'file_name']}\n",
    "        else:\n",
    "            return {\"im\" : im, \"labels\" : label_tensor}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Channel wise mean and standard deviation for normalizing according to ImageNet Statistics\n",
    "means = [0.485, 0.456, 0.406]\n",
    "stds = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "# Transforms to be applied to Train-Test-Validation\n",
    "train_transforms = transforms.Compose([\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(means, stds)\n",
    "    ])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(means, stds)])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(means, stds)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train, test and validation datasets are instantiated and wrapped around a ```DataLoader``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FlowerDataset(annotations=\"training.csv\",\n",
    "                                   data_path='./flower_dataset/train',\n",
    "                                   transform=train_transforms,\n",
    "                                   mapping=mapping)\n",
    "\n",
    "val_dataset = FlowerDataset(annotations=\"validation.csv\",\n",
    "                              data_path='./flower_dataset/valid',\n",
    "                              transform=val_transforms,\n",
    "                              mapping=mapping)\n",
    "\n",
    "test_dataset = FlowerDataset(annotations=\"testing.csv\",\n",
    "                              data_path='./flower_dataset/test',\n",
    "                              transform=test_transform,\n",
    "                             mapping=mapping,\n",
    "                              mode=\"test\")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=1,shuffle=False, num_workers=2)\n",
    "\n",
    "data_loader = {\"train\" : train_loader, \"valid\": val_loader}\n",
    "len_dict = {\"train\" : len(train_dataset), \"valid\" : len(val_dataset)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "The pre-trained weights of the ```ResNet-18``` model with ImageNet are used in this tutorial.\n",
    "\n",
    "To train the model, the following details are passed to the ```train_model()``` function\n",
    "\n",
    "1. **Model:** The edited version of the pre-trained model.\n",
    "2. **Data Loaders:** The dictionary containing our training and validation dataloaders\n",
    "3. **Criterion:** The loss function used for training the network\n",
    "4. **Num_epochs:** The number of epochs for which we would like to train the network.\n",
    "5. **dataset_size:** an additional parameter which is used to correctly scale the loss, the method for this is specified in the DataLoader cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83,
     "referenced_widgets": [
      "1e193402cc1b4dbe97531c28ac93a8bb",
      "18d04c743bb4459d8a8235e8846b178e",
      "ec07bf4e45574e9ca39217f19a0adb48",
      "822d18ace03e4637892cf40ae0996e4a",
      "ca38e79005dd4612ae97645c53113ae2",
      "2690eec5fed040cd801e83c2f178d068",
      "b37da2626af94b649d19213f38e14d65",
      "403ac43de7454dd4b4dfdab3a61ffe14"
     ]
    },
    "colab_type": "code",
    "id": "lCxV4MgZ8fEs",
    "outputId": "325236d0-57df-49f3-b881-a81183de8559"
   },
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freezing the weights\n",
    "for param in model.parameters():\n",
    "    param.required_grad = False\n",
    "\n",
    "\n",
    "# Replacing the final layer\n",
    "model.fc = nn.Sequential(nn.Linear(512, 256), nn.ReLU(), nn.Dropout(p=0.5), nn.Linear(256, 102),\n",
    "                         nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_usNSOE7Ao6n"
   },
   "outputs": [],
   "source": [
    "def train_model(model, data_loaders, optimizer, criterion, num_epochs, dataset_size):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Checks if GPU is present\n",
    "\n",
    "    model.to(device) # This method pushes the model to the device.\n",
    "    \n",
    "    # The training loop trains the model for the total number of epochs,\n",
    "    # an epoch is one complete pass over the entire dataset\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train() # This sets the model back to training after the validation step\n",
    "        print(\"Epoch Number {}\".format(epoch))\n",
    "\n",
    "        training_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        val_acc = 0\n",
    "        correct_preds = 0\n",
    "        best_acc = 0\n",
    "        validation = 0.0\n",
    "        total = 0\n",
    "\n",
    "        \n",
    "        data_loader = tqdm.tqdm(data_loaders[\"train\"])\n",
    "        for x, data in enumerate(data_loader):\n",
    "            inputs, labels = data[\"im\"].to(device), data[\"labels\"].to(device)\n",
    "            outputs = model(inputs)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            training_loss += loss.item()\n",
    "        \n",
    "        epoch_loss = training_loss / dataset_size[\"train\"]\n",
    "\n",
    "        print(\"Training Loss : {:.5f}\".format(epoch_loss))\n",
    "\n",
    "        val_data_loader = tqdm.tqdm(data_loaders[\"valid\"])\n",
    "        \n",
    "        # Validation step after every epoch\n",
    "        # The gradients are not required at inference time, hence the model is set to eval mode\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for x, data in enumerate(val_data_loader):\n",
    "                inputs, labels = data[\"im\"].to(device), data[\"labels\"].to(device)\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                val_loss = criterion(outputs, labels)\n",
    "                _, index = torch.max(outputs, 1)\n",
    "\n",
    "                total += labels.size(0)\n",
    "                correct_preds += (index == labels).sum().item()\n",
    "\n",
    "                validation += val_loss.item()\n",
    "\n",
    "            val_acc = 100 * (correct_preds / total)\n",
    "\n",
    "            print(\"Validation Loss : {:.5f}\".format(validation / dataset_size[\"valid\"]))\n",
    "            print(\"Validation Accuracy is: {:.2f}%\".format(val_acc))\n",
    "            \n",
    "            # The model is saved only if current validation accuracy is higher than the previous best accuracy\n",
    "            if best_acc < val_acc:\n",
    "                best_acc = val_acc\n",
    "                model_name = \"./saved_model_{:.2f}.pt\".format(best_acc)\n",
    "                torch.save(model, model_name)\n",
    "                \n",
    "            return model_name \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sKs3wDOaDmc0",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "0%|          | 0/103 [00:00<?, ?it/s]Epoch Number 0\n100%|██████████| 103/103 [09:17<00:00,  5.42s/it]\n  0%|          | 0/13 [00:00<?, ?it/s]Training Loss : 0.06202\n100%|██████████| 13/13 [00:23<00:00,  1.79s/it]Validation Loss : 0.04312\nValidation Accuracy is: 45.48%\n\n"
    }
   ],
   "source": [
    "# We use the Adam optimizer, which inherits the parameters of only the trainable layers.\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "trained_model = train_model(model=model, data_loaders=data_loader, optimizer=optimizer, num_epochs=1, dataset_size=len_dict, criterion=nn.NLLLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oASN5m6Ibn9l"
   },
   "outputs": [],
   "source": [
    "def test_model(dataloader, model):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    test_model = torch.load(model)\n",
    "    \n",
    "    test_model.eval()\n",
    "    tk0 = tqdm.tqdm(dataloader)\n",
    "    \n",
    "    total = 0\n",
    "    correct_preds = 0\n",
    "    pred_list = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, data in enumerate(tk0):\n",
    "            single_im, label = data[\"im\"].to(device), data[\"labels\"].to(device)\n",
    "            im_name = data[\"im_name\"]\n",
    "            \n",
    "            pred = test_model(single_im)\n",
    "\n",
    "            _, index = torch.max(pred, 1)\n",
    "\n",
    "            total += label.size(0)\n",
    "            correct_preds += (index == label).sum().item()\n",
    "            \n",
    "            pred_list[im_name[0]] = (index+1).item()\n",
    "            \n",
    "    df = pd.DataFrame(pred_list.items(), columns=['file_name', 'class_name'])\n",
    "    with open(\"results.csv\", \"w\") as f:\n",
    "        df.to_csv(f, index=False)\n",
    "    print('Accuracy of the network on the test images: %d %%' % (100 * (correct_preds / total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kkZFRcSncgFa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_model (test_loader, trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Predictions\n",
    "\n",
    "For visualizing the predicted v/s original label in Remo, the predictions are added to a CSV, which is then added as an ```AnnotationSet``` to the testing_dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Acquiring data - completed                                                                           \nProcessing data - Processing annotation files: 1/1Processing data - completed                                                                          \nData upload completed\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'session_id': '1b15014d-1f12-43b8-8b37-8288d73bd73c',\n 'created_at': '2020-07-22T12:03:30.043775Z',\n 'finished_at': '2020-07-22T12:04:33.366098Z',\n 'dataset': {'id': 38, 'name': 'flowers-test'},\n 'status': 'done',\n 'substatus': '',\n 'images': {'pending': 0,\n  'total': 0,\n  'successful': 0,\n  'failed': 0,\n  'errors': []},\n 'annotations': {'pending': 0,\n  'total': 1,\n  'successful': 1,\n  'failed': 0,\n  'errors': []},\n 'errors': [],\n 'uploaded': {'total': {'items': 0, 'size': 0, 'human_size': '0 b'},\n  'images': {'items': 0, 'size': 0, 'human_size': '0 b'},\n  'annotations': {'items': 0, 'size': 0, 'human_size': '0 b'},\n  'archives': {'items': 0, 'size': 0, 'human_size': '0 b'}}}"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "classes = [i for i in mapping.keys()]\n",
    "annotation_set = remo.create_annotation_set(\"Image Classification\", name=\"model_predictions\", dataset_id=<enter test dataset ID>, classes=classes)\n",
    "testing_dataset.add_data(local_files=[\"/home/harsha/Documents/rediscovery/image_classification/flower_classification/results.csv\"], annotation_set_id=annotation_set.id, class_encoding=cat_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Open http://localhost:8123/datasets/38\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n        <iframe\n            id=\"remo_frame_aea07e94-75dd-45f0-ad15-a033aef38ac9\"\n            width=\"100%\"\n            height=\"100px\"\n            src=\"http://localhost:8123/datasets/38?allheadless\"\n            frameborder=\"0\"\n            allowfullscreen\n        ></iframe>\n        <script type=\"text/javascript\">\n            (function () {\n                const iframe = document.getElementById(\"remo_frame_aea07e94-75dd-45f0-ad15-a033aef38ac9\");\n                let timeout, delay = 100;\n            \n                const setHeight = () => {\n                  const width = iframe.clientWidth;\n                  iframe.style.height = (width * screen.height / screen.width) * 0.8 + 'px';\n                }\n                window.addEventListener(\"resize\", () => {\n                    clearTimeout(timeout);\n                  // start timing for event \"completion\"\n                  timeout = setTimeout(setHeight, delay);\n                });\n                setHeight();\n            })()\n        </script>\n        "
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "testing_dataset.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "remo_image_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('pt_36': conda)",
   "language": "python",
   "name": "python361064bitpt36condae3e1a72d28c3471788eb7ee1d404ab9a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "18d04c743bb4459d8a8235e8846b178e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e193402cc1b4dbe97531c28ac93a8bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ec07bf4e45574e9ca39217f19a0adb48",
       "IPY_MODEL_822d18ace03e4637892cf40ae0996e4a"
      ],
      "layout": "IPY_MODEL_18d04c743bb4459d8a8235e8846b178e"
     }
    },
    "2690eec5fed040cd801e83c2f178d068": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "403ac43de7454dd4b4dfdab3a61ffe14": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "822d18ace03e4637892cf40ae0996e4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_403ac43de7454dd4b4dfdab3a61ffe14",
      "placeholder": "​",
      "style": "IPY_MODEL_b37da2626af94b649d19213f38e14d65",
      "value": " 44.7M/44.7M [00:21&lt;00:00, 2.16MB/s]"
     }
    },
    "b37da2626af94b649d19213f38e14d65": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ca38e79005dd4612ae97645c53113ae2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "ec07bf4e45574e9ca39217f19a0adb48": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2690eec5fed040cd801e83c2f178d068",
      "max": 46827520,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ca38e79005dd4612ae97645c53113ae2",
      "value": 46827520
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}